"Why try all parameters when math can tell you which ones matter?" â€“ Fermat (probably)

This repo contains **three theoretically-supercharged ML algorithms**   
1. **Elastic Net Regression** ðŸŽ¯: Tune regularization params smarter, not harder (O(p/ÎµÂ²) samples, not grid search!)  
2. **Logistic Classifier** ðŸ”®: Binary classification with guaranteed "probably okay-ish" results (math said so!)  
3. **Kernel Regression** ðŸŒ€: Non-linear modeling without summoning Cthulhu from high dimensions  

All come with **sample complexity guarantees** that would make Vapnik proud (or mildly annoyed).  

Inspired by https://blog.ml.cmu.edu/2024/04/12/how-to-regularize-your-regression/ blog post!
